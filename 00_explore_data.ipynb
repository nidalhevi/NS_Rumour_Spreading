{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c6cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59cae1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path(\"all-rnr-annotated-threads\")\n",
    "stats = defaultdict(lambda: defaultdict(int))\n",
    "issues = defaultdict(list)\n",
    "sample_data = defaultdict(list)\n",
    "threads_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2a2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_files_clean(dir_path: Path):\n",
    "    \"\"\"Return only real JSONs (skip macOS AppleDouble like '._*').\"\"\"\n",
    "    return [p for p in dir_path.glob(\"*.json\") if not p.name.startswith(\"._\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4731b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_structure():\n",
    "    \"\"\"Map the directory structure and count elements\"\"\"\n",
    "    \n",
    "    # Find all event directories\n",
    "    if not root_path.exists():\n",
    "        issues['critical'].append(f\"Root path does not exist: {root_path}\")\n",
    "        return\n",
    "        \n",
    "    event_dirs = [d for d in root_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    for event_dir in event_dirs:\n",
    "        event_name = event_dir.name\n",
    "        stats['events'][event_name] = 0\n",
    "        \n",
    "        # Check for rumours and non-rumours subdirectories\n",
    "        for category in ['rumours', 'non-rumours']:\n",
    "            category_path = event_dir / category\n",
    "            \n",
    "            if not category_path.exists():\n",
    "                issues['missing_dirs'].append(f\"{event_name}/{category}\")\n",
    "                continue\n",
    "            \n",
    "            # Count thread folders\n",
    "            thread_folders = [d for d in category_path.iterdir() if d.is_dir()]\n",
    "            thread_count = len(thread_folders)\n",
    "            \n",
    "            stats['events'][event_name] += thread_count\n",
    "            stats['categories'][category] += thread_count\n",
    "            stats['total']['threads'] += thread_count\n",
    "            \n",
    "            # Store thread info for sampling\n",
    "            for thread_folder in thread_folders:\n",
    "                threads_info.append({\n",
    "                    'path': thread_folder,\n",
    "                    'event': event_name,\n",
    "                    'category': category,\n",
    "                    'thread_id': thread_folder.name\n",
    "                })\n",
    "    \n",
    "    print(f\"   Found {len(event_dirs)} events\")\n",
    "    print(f\"   Total threads: {stats['total']['threads']}\")\n",
    "    print(f\"   Rumours: {stats['categories']['rumours']}\")\n",
    "    print(f\"   Non-rumours: {stats['categories']['non-rumours']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea87ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_thread(thread_info: Dict):\n",
    "    \"\"\"Examine a single thread folder\"\"\"\n",
    "    thread_path = thread_info['path']\n",
    "    thread_id = thread_info['thread_id']\n",
    "    \n",
    "    thread_data = {\n",
    "        'thread_id': thread_id,\n",
    "        'event': thread_info['event'],\n",
    "        'category': thread_info['category'],\n",
    "        'files': {}\n",
    "    }\n",
    "    \n",
    "    # Check for expected files\n",
    "    expected_files = ['annotation.json', 'structure.json']\n",
    "    expected_dirs = ['reactions', 'source-tweets']\n",
    "    \n",
    "    for file_name in expected_files:\n",
    "        file_path = thread_path / file_name\n",
    "        if file_path.exists():\n",
    "            thread_data['files'][file_name] = 'present'\n",
    "            stats['files'][file_name] += 1\n",
    "            \n",
    "            # Load and sample JSON\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "                    data = json.load(f)\n",
    "                    if len(sample_data[file_name]) < 3:\n",
    "                        sample_data[file_name].append({\n",
    "                            'thread_id': thread_id,\n",
    "                            'data': data\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                issues['json_errors'].append(f\"{thread_id}/{file_name}: {str(e)}\")\n",
    "        else:\n",
    "            thread_data['files'][file_name] = 'missing'\n",
    "            issues['missing_files'].append(f\"{thread_id}/{file_name}\")\n",
    "    \n",
    "    # Check directories\n",
    "    for dir_name in expected_dirs:\n",
    "        dir_path = thread_path / dir_name\n",
    "        if dir_path.exists():\n",
    "            # json_files = list(dir_path.glob('*.json'))\n",
    "            json_files = _json_files_clean(dir_path) ### NEW ###\n",
    "            count = len(json_files)\n",
    "            thread_data['files'][dir_name] = f\"{count} files\"\n",
    "            stats['dir_counts'][dir_name] += count\n",
    "            \n",
    "            # Sample one JSON from each directory\n",
    "            # if json_files and len(sample_data[dir_name]) < 3:\n",
    "            #     try:\n",
    "            #         # with open(json_files[0], 'r', encoding='utf-8') as f:\n",
    "            #         with open(json_files[0], 'r', encoding='utf-8-sig') as f:\n",
    "            #             data = json.load(f)\n",
    "            #             sample_data[dir_name].append({\n",
    "            #                 'thread_id': thread_id,\n",
    "            #                 'file': json_files[0].name,\n",
    "            #                 'data': data\n",
    "            #             })\n",
    "            #     except Exception as e:\n",
    "            #         issues['json_errors'].append(f\"{thread_id}/{dir_name}/{json_files[0].name}: {str(e)}\")\n",
    "            \n",
    "            ### NEW ###\n",
    "            if json_files and len(sample_data[dir_name]) < 3:\n",
    "                try:\n",
    "                    # pick the first real JSON after filtering; could also sort by name/time if you prefer\n",
    "                    jf = json_files[0]\n",
    "                    with open(jf, 'r', encoding='utf-8-sig') as f:\n",
    "                        data = json.load(f)\n",
    "                    sample_data[dir_name].append({\n",
    "                        'thread_id': thread_id,\n",
    "                        'file': jf.name,\n",
    "                        'data': data\n",
    "                    })\n",
    "                except UnicodeDecodeError as e:\n",
    "                    issues['json_errors'].append(f\"{thread_id}/{dir_name}/{jf.name}: {e}\")\n",
    "                except Exception as e:\n",
    "                    issues['json_errors'].append(f\"{thread_id}/{dir_name}/{jf.name}: {str(e)}\")\n",
    "\n",
    "            ### END NEW ###\n",
    "        else:\n",
    "            thread_data['files'][dir_name] = 'missing'\n",
    "            issues['missing_dirs'].append(f\"{thread_id}/{dir_name}\")\n",
    "    \n",
    "    # Store sample thread data\n",
    "    if len(sample_data['threads']) < 10:\n",
    "        sample_data['threads'].append(thread_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9761e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_threads(sample_size: int):\n",
    "    \"\"\"Sample threads and examine their JSON structures\"\"\"\n",
    "    \n",
    "    # Sample from each event/category combination\n",
    "    for event_name in set(t['event'] for t in threads_info):\n",
    "        for category in ['rumours', 'non-rumours']:\n",
    "            threads = [t for t in threads_info \n",
    "                      if t['event'] == event_name and t['category'] == category]\n",
    "            \n",
    "            sample = threads[:min(sample_size, len(threads))]\n",
    "            \n",
    "            for thread in sample:\n",
    "                examine_thread(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea555e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_timestamps():\n",
    "    \"\"\"Check timestamp formats across different JSON files\"\"\"\n",
    "    timestamp_formats = Counter()\n",
    "    \n",
    "    # Check source tweets\n",
    "    for sample in sample_data['source-tweets']:\n",
    "        data = sample['data']\n",
    "        if 'created_at' in data:\n",
    "            ts = data['created_at']\n",
    "            timestamp_formats[type(ts).__name__] += 1\n",
    "            # Try to parse\n",
    "            try:\n",
    "                if isinstance(ts, str):\n",
    "                    # Twitter format: \"Wed Oct 10 20:19:24 +0000 2018\"\n",
    "                    datetime.strptime(ts, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "            except:\n",
    "                issues['timestamp_parsing'].append(f\"Source tweet: {ts}\")\n",
    "    \n",
    "    # Check reactions\n",
    "    for sample in sample_data['reactions']:\n",
    "        data = sample['data']\n",
    "        if 'created_at' in data:\n",
    "            ts = data['created_at']\n",
    "            try:\n",
    "                if isinstance(ts, str):\n",
    "                    datetime.strptime(ts, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "            except:\n",
    "                issues['timestamp_parsing'].append(f\"Reaction: {ts}\")\n",
    "    \n",
    "    stats['timestamp_formats'] = dict(timestamp_formats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e211a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_user_ids():\n",
    "    \"\"\"Check user ID formats and consistency\"\"\"\n",
    "    user_id_types = Counter()\n",
    "    \n",
    "    for sample in sample_data['source-tweets']:\n",
    "        data = sample['data']\n",
    "        if 'user' in data and 'id' in data['user']:\n",
    "            user_id_types[type(data['user']['id']).__name__] += 1\n",
    "    \n",
    "    stats['user_id_types'] = dict(user_id_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78abe9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_languages():\n",
    "    \"\"\"Check language distribution\"\"\"\n",
    "    languages = Counter()\n",
    "    \n",
    "    for sample in sample_data['source-tweets']:\n",
    "        data = sample['data']\n",
    "        if 'lang' in data:\n",
    "            languages[data['lang']] += 1\n",
    "    \n",
    "    for sample in sample_data['reactions']:\n",
    "        data = sample['data']\n",
    "        if 'lang' in data:\n",
    "            languages[data['lang']] += 1\n",
    "    \n",
    "    stats['languages'] = dict(languages.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96540d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_veracity_labels():\n",
    "    \"\"\"Check veracity label distribution\"\"\"\n",
    "    veracity_labels = Counter()\n",
    "    \n",
    "    for sample in sample_data['annotation.json']:\n",
    "        data = sample['data']\n",
    "        if 'true' in data:\n",
    "            label = data['true']\n",
    "            veracity_labels[str(label)] += 1\n",
    "    \n",
    "    stats['veracity_labels'] = dict(veracity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488b2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_quality():\n",
    "    \"\"\"Perform data quality checks\"\"\"\n",
    "    # Check timestamp formats\n",
    "    check_timestamps()\n",
    "    \n",
    "    # Check for user ID consistency\n",
    "    check_user_ids()\n",
    "    \n",
    "    # Check language distribution\n",
    "    check_languages()\n",
    "    \n",
    "    # Check veracity labels\n",
    "    check_veracity_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e71b21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report():\n",
    "    \"\"\"Generate and save comprehensive report\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"Dataset Exploration\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "    # 1. Overall Statistics\n",
    "    report.append(\"\\nOverall stats\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"Total threads: {stats['total']['threads']}\")\n",
    "    report.append(f\"Rumours: {stats['categories']['rumours']}\")\n",
    "    report.append(f\"Non-rumours: {stats['categories']['non-rumours']}\")\n",
    "    \n",
    "    # 2. Event Distribution\n",
    "    report.append(\"\\n\\n Event distribution\")\n",
    "    report.append(\"-\" * 80)\n",
    "    for event, count in sorted(stats['events'].items(), key=lambda x: x[1], reverse=True):\n",
    "        report.append(f\"  {event}: {count} threads\")\n",
    "    \n",
    "    # 3. File Presence\n",
    "    report.append(\"\\n\\nFile stats\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"annotation.json files: {stats['files'].get('annotation.json', 0)}\")\n",
    "    report.append(f\"structure.json files: {stats['files'].get('structure.json', 0)}\")\n",
    "    report.append(f\"Total reaction files: {stats['dir_counts'].get('reactions', 0)}\")\n",
    "    report.append(f\"Total source-tweet files: {stats['dir_counts'].get('source-tweets', 0)}\")\n",
    "    \n",
    "    # 4. Data Quality Issues\n",
    "    report.append(\"\\n\\n ⚠️  Data quality\")\n",
    "    report.append(\"-\" * 80)\n",
    "    if not any(issues.values()):\n",
    "        report.append(\"  No issues found!\")\n",
    "    else:\n",
    "        for issue_type, issue_list in issues.items():\n",
    "            if issue_list:\n",
    "                report.append(f\"\\n  {issue_type.upper()}: {len(issue_list)} issues\")\n",
    "                for issue in issue_list[:5]:  # Show first 5\n",
    "                    report.append(f\"    - {issue}\")\n",
    "                if len(issue_list) > 5:\n",
    "                    report.append(f\"    ... and {len(issue_list) - 5} more\")\n",
    "    \n",
    "    # 5. Language Distribution\n",
    "    if stats['languages']:\n",
    "        report.append(\"\\n\\n Language distribution\")\n",
    "        report.append(\"-\" * 80)\n",
    "        for lang, count in stats['languages'].items():\n",
    "            report.append(f\"  {lang}: {count}\")\n",
    "    \n",
    "    # 6. Veracity Labels\n",
    "    if stats['veracity_labels']:\n",
    "        report.append(\"\\n\\n Veracity labels\")\n",
    "        report.append(\"-\" * 80)\n",
    "        for label, count in stats['veracity_labels'].items():\n",
    "            report.append(f\"  {label}: {count}\")\n",
    "    \n",
    "    # 7. JSON Schema Samples\n",
    "    report.append(\"\\n\\nSample json schema\")\n",
    "    report.append(\"-\" * 80)\n",
    "    \n",
    "    # annotation.json sample\n",
    "    if sample_data['annotation.json']:\n",
    "        report.append(\"\\n  annotation.json structure:\")\n",
    "        sample = sample_data['annotation.json'][0]['data']\n",
    "        report.append(f\"    Keys: {list(sample.keys())}\")\n",
    "    \n",
    "    # source-tweets sample\n",
    "    if sample_data['source-tweets']:\n",
    "        report.append(\"\\n  source-tweets/*.json structure:\")\n",
    "        sample = sample_data['source-tweets'][0]['data']\n",
    "        report.append(f\"    Keys: {list(sample.keys())}\")\n",
    "        if 'user' in sample:\n",
    "            report.append(f\"    User keys: {list(sample['user'].keys())}\")\n",
    "    \n",
    "    # reactions sample\n",
    "    if sample_data['reactions']:\n",
    "        report.append(\"\\n  reactions/*.json structure:\")\n",
    "        sample = sample_data['reactions'][0]['data']\n",
    "        report.append(f\"    Keys: {list(sample.keys())}\")\n",
    "    \n",
    "    # 8. Sample Thread Details\n",
    "    report.append(\"\\n\\nSample thread\")\n",
    "    report.append(\"-\" * 80)\n",
    "    for thread in sample_data['threads'][:3]:\n",
    "        report.append(f\"\\n  Thread: {thread['thread_id']}\")\n",
    "        report.append(f\"    Event: {thread['event']}\")\n",
    "        report.append(f\"    Category: {thread['category']}\")\n",
    "        report.append(f\"    Files:\")\n",
    "        for file, status in thread['files'].items():\n",
    "            report.append(f\"      - {file}: {status}\")\n",
    "    \n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    report.append(\"End of report\")\n",
    "    report.append(\"=\" * 80)\n",
    "    \n",
    "    # Print report\n",
    "    report_text = \"\\n\".join(report)\n",
    "    print(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8decc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(sample_size: int = 5):\n",
    "    \"\"\"Main exploration function\"\"\"\n",
    "    print(f\" Starting PHEME Dataset Exploration...\")\n",
    "    print(f\" Root directory: {root_path}\\n\")\n",
    "    \n",
    "    # Verify root exists\n",
    "    if not root_path.exists():\n",
    "        print(f\"ERROR: Path {root_path} does not exist!\")\n",
    "        return\n",
    "    \n",
    "    # Step 1: Map directory structure\n",
    "    print(\"\\nStep 1: Mapping directory structure...\")\n",
    "    map_structure()\n",
    "    \n",
    "    # Step 2: Sample and examine JSONs\n",
    "    print(f\"\\nStep 2: Sampling {sample_size} threads per category...\")\n",
    "    sample_threads(sample_size)\n",
    "    \n",
    "    # Step 3: Identify data quality issues\n",
    "    print(\"\\nStep 3: Checking data quality...\")\n",
    "    check_quality()\n",
    "    \n",
    "    # Step 4: Generate report\n",
    "    print(\"\\n Step 4: Generating report...\")\n",
    "    generate_report()\n",
    "    \n",
    "    print(\"\\n Exploration complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d7c96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting PHEME Dataset Exploration...\n",
      " Root directory: all-rnr-annotated-threads\n",
      "\n",
      "\n",
      "Step 1: Mapping directory structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 9 events\n",
      "   Total threads: 6425\n",
      "   Rumours: 2402\n",
      "   Non-rumours: 4023\n",
      "\n",
      "Step 2: Sampling 5 threads per category...\n",
      "\n",
      "Step 3: Checking data quality...\n",
      "\n",
      " Step 4: Generating report...\n",
      "================================================================================\n",
      "Dataset Exploration\n",
      "================================================================================\n",
      "Generated: 2025-12-11 17:06:29\n",
      "\n",
      "\n",
      "Overall stats\n",
      "--------------------------------------------------------------------------------\n",
      "Total threads: 6425\n",
      "Rumours: 2402\n",
      "Non-rumours: 4023\n",
      "\n",
      "\n",
      " Event distribution\n",
      "--------------------------------------------------------------------------------\n",
      "  charliehebdo-all-rnr-threads: 2079 threads\n",
      "  sydneysiege-all-rnr-threads: 1221 threads\n",
      "  ferguson-all-rnr-threads: 1143 threads\n",
      "  ottawashooting-all-rnr-threads: 890 threads\n",
      "  germanwings-crash-all-rnr-threads: 469 threads\n",
      "  putinmissing-all-rnr-threads: 238 threads\n",
      "  prince-toronto-all-rnr-threads: 233 threads\n",
      "  gurlitt-all-rnr-threads: 138 threads\n",
      "  ebola-essien-all-rnr-threads: 14 threads\n",
      "\n",
      "\n",
      "File stats\n",
      "--------------------------------------------------------------------------------\n",
      "annotation.json files: 84\n",
      "structure.json files: 84\n",
      "Total reaction files: 983\n",
      "Total source-tweet files: 84\n",
      "\n",
      "\n",
      " ⚠️  Data quality\n",
      "--------------------------------------------------------------------------------\n",
      "  No issues found!\n",
      "\n",
      "\n",
      " Language distribution\n",
      "--------------------------------------------------------------------------------\n",
      "  en: 6\n",
      "\n",
      "\n",
      " Veracity labels\n",
      "--------------------------------------------------------------------------------\n",
      "  1: 3\n",
      "\n",
      "\n",
      "Sample json schema\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  annotation.json structure:\n",
      "    Keys: ['is_rumour', 'category', 'misinformation', 'true', 'links', 'is_turnaround']\n",
      "\n",
      "  source-tweets/*.json structure:\n",
      "    Keys: ['contributors', 'truncated', 'text', 'in_reply_to_status_id', 'id', 'favorite_count', 'source', 'retweeted', 'coordinates', 'entities', 'in_reply_to_screen_name', 'id_str', 'retweet_count', 'in_reply_to_user_id', 'favorited', 'user', 'geo', 'in_reply_to_user_id_str', 'possibly_sensitive', 'lang', 'created_at', 'filter_level', 'in_reply_to_status_id_str', 'place']\n",
      "    User keys: ['follow_request_sent', 'profile_use_background_image', 'default_profile_image', 'id', 'verified', 'profile_image_url_https', 'profile_sidebar_fill_color', 'profile_text_color', 'followers_count', 'profile_sidebar_border_color', 'id_str', 'profile_background_color', 'listed_count', 'profile_background_image_url_https', 'utc_offset', 'statuses_count', 'description', 'friends_count', 'location', 'profile_link_color', 'profile_image_url', 'following', 'geo_enabled', 'profile_background_image_url', 'name', 'lang', 'profile_background_tile', 'favourites_count', 'screen_name', 'notifications', 'url', 'created_at', 'contributors_enabled', 'time_zone', 'protected', 'default_profile', 'is_translator']\n",
      "\n",
      "  reactions/*.json structure:\n",
      "    Keys: ['contributors', 'truncated', 'text', 'in_reply_to_status_id', 'id', 'favorite_count', 'source', 'retweeted', 'coordinates', 'entities', 'in_reply_to_screen_name', 'id_str', 'retweet_count', 'in_reply_to_user_id', 'favorited', 'user', 'geo', 'in_reply_to_user_id_str', 'lang', 'created_at', 'in_reply_to_status_id_str', 'place']\n",
      "\n",
      "\n",
      "Sample thread\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Thread: 524922507380670464\n",
      "    Event: ottawashooting-all-rnr-threads\n",
      "    Category: rumours\n",
      "    Files:\n",
      "      - annotation.json: present\n",
      "      - structure.json: present\n",
      "      - reactions: 6 files\n",
      "      - source-tweets: 1 files\n",
      "\n",
      "  Thread: 524922729485848576\n",
      "    Event: ottawashooting-all-rnr-threads\n",
      "    Category: rumours\n",
      "    Files:\n",
      "      - annotation.json: present\n",
      "      - structure.json: present\n",
      "      - reactions: 36 files\n",
      "      - source-tweets: 1 files\n",
      "\n",
      "  Thread: 524923293711998976\n",
      "    Event: ottawashooting-all-rnr-threads\n",
      "    Category: rumours\n",
      "    Files:\n",
      "      - annotation.json: present\n",
      "      - structure.json: present\n",
      "      - reactions: 23 files\n",
      "      - source-tweets: 1 files\n",
      "\n",
      "================================================================================\n",
      "End of report\n",
      "================================================================================\n",
      "\n",
      " Exploration complete!\n"
     ]
    }
   ],
   "source": [
    "explore(sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a20b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c15222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d1323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
